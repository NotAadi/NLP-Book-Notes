{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# NLP Basic Tasks\n",
    "\n",
    "- __Tokenization__: The process of breaking text down into meaningful elements such as words or punctuation. Also involves turning text into tensors that neural nets can work with\n",
    "- __Part of Speech (POS) Tagging__: Identifying what grammatical part of speech (verb, proper noun, adjective, etc.) each token is\n",
    "- __Dependency Parsing__: Identifying relationships between tokens, creating a syntactic structure for the sentence\n",
    "- __Chunking__: Combining related tokens into a single token, such as related noun groups or related verb groups. E.g., Tokens of \"New\", \"York\", and \"City\" would be grouped into one token for \"New York City\"\n",
    "- __Lemmatization__: Converting words into their base, canonical forms. Can be compared to Stemming, which converts words into their stems and can sometimes lead to nonsensical sub-words\n",
    "- __Named Entity Recognition__: One step beyond POS Tagging, where labels are assigned to known entities such as locations, persons, organizations, and currencies\n",
    "- __Entity Linking__: Disambiguating entities with the use of context clues and external databases."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transfer Learning and Transformers\n",
    "Transfer Learning is the process of taking a pre-trained ML model and fine-tuning it to meet your specific use case. Transfer Learning also uses the logic that, since the prebuilt model you start from has been trained on a lot of diverse data, your fine-tuned model will have a lot more knowledge it can use at its disposal. We are using a type of Neural Net called a Transformer model, which is hard to build from scratch and easier to fine-tune with custom data.\n",
    "\n",
    "Transfer learning often happens off of Language Models, which are a type of NLP model that predicts the next word (or words) in a sentence given a starting point. These language models are then fine-tuned to create other types of NLP models such as text classification models. These models are the backbone of NLP since, to predict the next word in a sentence, you _must_ have a good understanding of the language. They are also important since they can be fine-tuned off raw text data instead of requiring hard-to-get labeled text data. You can do inference with language models using Hugging Face and its transformers library.\n",
    "\n",
    "# ULMFiT\n",
    "The naive approach to getting a fine-tuned classification model from a language model would be to convert that language model into a classification model, and then fine-tune the classification model on your specific data. However, a better approach is available, known as __ULMFiT__, where you fine-tune the language model first, AND THEN convert it to a classification model and fine tune that. This is better since the language model gets more used to the type of language that you're classifying before you convert and fine-tune it. ULMFiT also is better since it lets you fine-tune with raw data on a language model instead of having to use hard-to-find labeled text data to fine-tune a classification model.\n",
    "\n",
    "To do ULMFiT to get a fine-tuned classifier model, you do the following:\n",
    "1. Load your pretrained model into a language model\n",
    "2. Fine-tune this pretrained model using your dataset and save its __encoder__. The encoder is everything except for the decoder \"head\" of the model.\n",
    "3. Create a classifier model and load your saved encoder into this model\n",
    "4. Fine-tune this classifier model\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
